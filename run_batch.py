import os,sys
import ast
from operator import itemgetter
import shutil
import argparse
import inspect
from inspect import currentframe, getframeinfo

import numpy as np

from utils.MPI import MPI

import tie_breaker
from utils import indri
from utils.evaluation import Evaluation


_root = '../batch/'
output_root = '../all_results/'
max_nodes = 120


def get_already_generated_qids(collection_path):
    r = []
    fn = os.path.join(collection_path, 'detailed_doc_stats_log')
    if os.path.exists( fn ):
        with open(fn) as f:
            r = [line.strip() for line in f.readlines() if line.strip()]

    return r


def collect_existing_detailed_doc_stats_results(intermediate_results_root, 
        all_results_dict, already_generated_qids, final_output_path):
    """
    collect the parallel results generated by multiple nodes
    """
    if not os.path.exists(final_output_path):
        os.makedirs(final_output_path)

    final_results_only_docid = {}
    # final_results = {}
    # final_results_only_docid = {}
    # for fn in os.listdir(final_output_path):
    #     if fn not in already_generated_qids:
    #         with open(os.path.join(final_output_path, fn)) as f:
    #             for line in f:
    #                 line = line.strip()
    #                 if line:
    #                     row = line.split(',')
    #                     docid = row[1]
    #                     row[2] = ast.literal_eval(row[2])
    #                     if fn not in final_results_only_docid:
    #                         final_results_only_docid[fn] = []
    #                     final_results_only_docid[fn].append(docid)
    #                     if fn not in final_results:
    #                         final_results[fn] = []
    #                     final_results[fn].append(row)
    #     else:
    #         print 'Skipping[final results folder] qid:%s' % fn

    #print len(final_results)

    if os.path.exists(intermediate_results_root):
        for fn in os.listdir(intermediate_results_root):
            with open(os.path.join(intermediate_results_root, fn)) as f:
                for line in f:
                    line = line.strip()
                    if line:
                        try:
                            row = line.split(',')
                            qid = row[0]
                            did = row[1]
                            if qid in already_generated_qids:
                                print 'Skipping[intermediate results folder] qid:%s' % qid
                                continue
                                
                            # row[2] = ast.literal_eval(row[2])
                            # if qid in final_results and did in final_results[qid]:
                            #     #exists
                            #     pass
                            # else:
                            #     if qid not in final_results:
                            #         final_results[qid] = []
                            #     final_results[qid].append(row)
                            #     if qid not in final_results_only_docid:
                            #         final_results_only_docid[qid] = []
                            #     final_results_only_docid[qid].append(did)
                            if qid not in final_results_only_docid:
                                final_results_only_docid[qid] = []
                            final_results_only_docid[qid].append(did)
                            with open(os.path.join(final_output_path, qid), 'ab') as f:
                                f.write(line+'\n')
                        except:
                            print line

    # for qid in final_results:
    #     final_results[qid].sort(key=itemgetter(2), reverse=True)

    # for qid in final_results:
    #     with open(os.path.join(final_output_path, qid), 'wb') as f:
    #         for ele in final_results[qid]:
    #             ele[2] = str(ele[2])
    #             f.write(','.join(ele)+'\n')
    try:
        #shutil.rmtree(intermediate_results_root)
        pass
    except:
        print 'rmtree %s failed!' % intermediate_results_root


    all_results_dict_local = {qid:[ele[0] for ele in r] for qid,r in all_results_dict.items() if qid not in already_generated_qids}
    not_run = {}
    for qid, doc_ids in all_results_dict_local.items():
        if qid not in final_results_only_docid:
            not_run[qid] = all_results_dict[qid]
        else:
            not_run_list = list(set(doc_ids)-set(final_results_only_docid[qid]))
            if not_run_list:
                if qid not in not_run:
                    not_run[qid] = []
                for doc_id in not_run_list:
                    for ele in all_results_dict[qid]:
                        if ele[0] == doc_id:
                            not_run[qid].append(ele)
                            break
        if qid not in not_run or not not_run[qid]:
            already_generated_qids.append(qid)
    return not_run


def gen_detailed_doc_stats(result_file):
    if not os.path.exists(result_file):
        print 'Result File:', result_file, ' Does not exists!'
        exit()

    current_function_name = inspect.stack()[0][3]
    collection_path = os.path.abspath('/'.join(result_file.split('/')[:-2]))
    collection_name = collection_path.split('/')[-1]
    final_output_path = os.path.join(collection_path, "detailed_doc_stats")

    intermediate_results_root = os.path.abspath(os.path.join(_root, collection_name, 
        'intermediate_results', current_function_name))

    already_generated_qids = get_already_generated_qids(collection_path)
    all_doc_paras = tie_breaker.TieBreaker(collection_path).gen_doc_details(result_file)
    all_doc_paras = collect_existing_detailed_doc_stats_results(
        intermediate_results_root, all_doc_paras, already_generated_qids, final_output_path)

    already_generated_qids = list(set(already_generated_qids))
    with open(os.path.join(collection_path, 'detailed_doc_stats_log'), 'wb') as f:
        f.write('\n'.join(already_generated_qids))

    #print all_doc_paras.keys(), len(all_doc_paras)
    if not all_doc_paras:
        print 'Nothing to RUN'
    else:
        all_paras = []
        for qid in all_doc_paras:
            all_paras.append([])
            for ele in all_doc_paras[qid]:
                all_paras[-1].append([collection_path, qid, ele[0], str(ele[1])])

        query_cnt = len(all_doc_paras)

        # split the paras based on the qid so that we can get best parallel efficiency!
        total_nodes = min( max_nodes-1, query_cnt )
        while len(all_paras)>total_nodes:
            tmp = []
            for i in range(0,len(all_paras),2):
                if i+1 < len(all_paras):
                    tmp.append(all_paras[i]+all_paras[i+1])
            all_paras = tmp
        # split the paras based on the qid so that we can get best parallel efficiency!

        #print all_paras, len(all_paras)
        if not os.path.exists(intermediate_results_root):
            os.makedirs(intermediate_results_root)

        MPI().gen_batch_framework(os.path.join(_root, collection_name, 'bin'), 
            current_function_name, "tie_breaker.py", '11', 
            all_paras, 
            os.path.join(_root, collection_name, 'misc', current_function_name), 
            para_alreay_split=True,
            add_node_to_para=True, node_para_prefix=intermediate_results_root,
            run_after_gen=False, max_nodes=total_nodes, memory='4G')
  

###################################################
def run_all_baseline_results_atom(para_file):
    with open(para_file) as f:
        for line in f:
            line = line.strip()
            if line:
                row = line.split()
                indri.IndriRunQuery(row[0], row[2], row[1])


def run_all_baseline_results(collections=[]):
    frameinfo = getframeinfo(currentframe())
    current_function_name = inspect.stack()[0][3]
    result_dir = 'all_baseline_results'
    all_paras = []

    for c in collections:
        collection_name = c.split('/')[-1]
        if not collection_name:
            continue
        if not os.path.exists( os.path.join(c, result_dir) ):
            os.makedirs( os.path.join(c, result_dir) )
        q_path = os.path.join(os.path.abspath(c), 'standard_queries')
        for bs in np.arange(0., 1.01, 0.05):
            r_path = os.path.join(os.path.abspath(c), result_dir, 'pivoted_'+str(bs))
            if not os.path.exists(r_path):
                all_paras.append((q_path, '-rule=method:pivoted,s:%s' % bs, r_path))
            r_path = os.path.join(os.path.abspath(c), result_dir, 'okapi_'+str(bs))
            if not os.path.exists(r_path):
                all_paras.append((q_path, '-rule=method:okapi,b:%s' % bs, r_path))

        for miu in range(0,5001,250):
            r_path = os.path.join(os.path.abspath(c), result_dir, 'lm_'+str(miu))
            if not os.path.exists(r_path):
                all_paras.append((q_path, '-rule=method:d,mu:%s' % miu, r_path))

        if all_paras:
            #print all_paras
            MPI().gen_batch_framework(os.path.join(_root, collection_name, 'bin'), 
                current_function_name, frameinfo.filename, '21', 
                all_paras, 
                os.path.join(_root, collection_name, 'misc', current_function_name), 
                para_alreay_split=False,
                add_node_to_para=False,
                run_after_gen=True,
                memory='1G'
            )
        else:
            print 'Nothing to RUN for '+c

def gen_baseline_best_results(collections=[]):
    output_folder = 'baselines'
    if not os.path.exists( os.path.join(output_root, output_folder) ):
        os.makedirs( os.path.join(output_root, output_folder) )
    for c in collections:
        collection_name = c.split('/')[-1] if  c.split('/')[-1] else  c.split('/')[-2]
        MAP = {}
        for fn in os.listdir(os.path.join(c, 'all_baseline_results')):
            method = fn.split('_')[0] 
            evaluation = Evaluation(c, os.path.join(c, 'all_baseline_results', fn)).get_all_performance(return_all_metrics=True, metrics=['map'])
            for qid in evaluation:
                if qid not in MAP:
                    MAP[qid] = {}
                if method not in MAP[qid]:
                    MAP[qid][method] = {}
                MAP[qid][method][fn] = evaluation[qid]['map']
        MAP1 = {}
        for qid in MAP:
            MAP1[qid] = []
            for method in MAP[qid]:
                MAP1[qid].append(sorted(MAP[qid][method].items(), key=itemgetter(1), reverse=True)[0])

        with open(os.path.join(output_root, output_folder, collection_name+'-all_baseline_results.csv'), 'wb') as output:
            for qid in sorted(MAP1.keys()):
                output.write('%s' % qid)
                for ele in MAP1[qid]:
                    output.write(',%s,%s' % (ele[0], ele[1]))
                output.write('\n')



if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument("-1", "--gen_detailed_doc_stats",
        nargs=1,
        help="Generate the detailed document for the query. The input is \
            the result file.")

    parser.add_argument("-2", "--run_all_baseline_results",
        nargs='+',
        help="Run all parameters for Pivoted, Okapi and Dirichlet.")

    parser.add_argument("-21", "--run_all_baseline_results_atom",
        nargs=1,
        help="Run all parameters for Pivoted, Okapi and Dirichlet. This actually does the work.")

    parser.add_argument("-22", "--gen_baseline_best_results",
        nargs='+',
        help="Output the baseline best results.")

    args = parser.parse_args()

    if args.gen_detailed_doc_stats:
        gen_detailed_doc_stats(args.gen_detailed_doc_stats[0])

    if args.run_all_baseline_results:
        run_all_baseline_results(args.run_all_baseline_results)

    if args.run_all_baseline_results_atom:
        run_all_baseline_results_atom(args.run_all_baseline_results_atom[0])

    if args.gen_baseline_best_results:
        gen_baseline_best_results(args.gen_baseline_best_results)    


